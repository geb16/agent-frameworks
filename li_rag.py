"""
li_rag.py
-------------
LlamaIndex RAG (Document Indexing + Query Engine) for policy Q&A.

Loads documents, builds a vector index, and answers questions using LlamaIndex and OpenAI.

Usage:
    python li_rag.py

References:
    - https://docs.llamaindex.ai/
    - https://platform.openai.com/docs/
    - https://python.langchain.com/docs/
    - https://docs.trychroma.com/
"""
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings
from llama_index.core.node_parser import SentenceSplitter
from llama_index.llms.openai import OpenAI
from dotenv import load_dotenv


load_dotenv()

# Configure via Settings (replaces ServiceContext and SimpleNodeParser)
Settings.llm = OpenAI(model="gpt-4o-mini", temperature=0.2)
Settings.text_splitter = SentenceSplitter(chunk_size=300, chunk_overlap=50)

documents = SimpleDirectoryReader("data").load_data()
index = VectorStoreIndex.from_documents(documents)

query_engine = index.as_query_engine(similarity_top_k=3)


def query_policy(question: str) -> str:
    """
    Query the policy index for an answer to the given question.

    Args:
        question (str): The user's question.

    Returns:
        str: The answer generated by the query engine.
    """
    response = query_engine.query(question)
    return str(response)

if __name__ == "__main__":
    print(query_policy("What is the refund policy?"))


