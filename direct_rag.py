"""
direct_rag.py
-------------
Minimal RAG implementation without frameworks, using ChromaDB and OpenAI.

Embeds and stores documents, retrieves context, and answers questions using only the provided context.

Usage:
    python direct_rag.py

References:
    - https://platform.openai.com/docs/
    - https://docs.trychroma.com/
    - https://python.langchain.com/docs/
    - https://docs.llamaindex.ai/
"""
import chromadb
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# load DB
db = chromadb.PersistentClient(path="./db")
collection = db.get_or_create_collection("docs")

def embed(text):
    """
    Generate an embedding vector for the given text using OpenAI.

    Args:
        text (str): The text to embed.

    Returns:
        list[float]: The embedding vector.
    """
    resp = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return resp.data[0].embedding

def rag(query):
    """
    Run a minimal RAG pipeline: embed the query, retrieve context, and answer using OpenAI chat.

    Args:
        query (str): The user's question.

    Returns:
        str: The answer generated by the model.
    """
    q_vec = embed(query)
    results = collection.query(
        query_embeddings=[q_vec],
        n_results=3
    )

    documents = results.get("documents")
    if documents and documents[0]:
        context = "\n\n".join(documents[0])
    else:
        context = "No relevant documents found."

    prompt = f"""
Answer using ONLY this context.

CONTEXT:
{context}

QUESTION:
{query}
"""

    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role":"user","content":prompt}],
        temperature=0.2
    )

    return resp.choices[0].message.content


if __name__ == "__main__":
    print(rag("What is the refund policy?"))
